{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11ab5bcb-e8ae-4491-aa14-5c24a8d7fd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (raw/used): (800, 5) / (800, 3)\n",
      "Init membership shape: (800, 3)\n",
      "True posterior shape : (800, 3)\n",
      "Inferred clusters C = 3\n",
      "[System] Selected device: cuda\n",
      "[DDQN] Using device: cuda | param device: cuda:0\n",
      "[DDQN] Params on cuda: False\n",
      "[Check] Q tensor device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   0%|                                | 0/150 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] state=('Stalled', 'VLD') -> action p_m=0.75, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   1%|▏                   | 1/150 [04:21<10:50:25, 261.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.000000 diversity=4.1004 reward=0\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.25, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   1%|▎                   | 2/150 [08:45<10:48:34, 262.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.050311 diversity=3.3361 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   2%|▍                   | 3/150 [13:13<10:49:24, 265.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.031356 diversity=2.8846 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   3%|▌                   | 4/150 [17:32<10:39:54, 262.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.098632 diversity=4.6635 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   3%|▋                   | 5/150 [21:52<10:32:09, 261.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.070493 diversity=4.7226 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   4%|▊                   | 6/150 [26:10<10:24:51, 260.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.003121 diversity=5.5812 reward=150\n",
      "[DDQN] state=('HC', 'VHD') -> action p_m=0.75, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   5%|▉                   | 7/150 [30:27<10:18:34, 259.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.010191 diversity=7.2082 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   5%|█                   | 8/150 [34:48<10:14:46, 259.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.012880 diversity=1.3646 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   6%|█▏                  | 9/150 [39:06<10:09:19, 259.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.004936 diversity=5.2078 reward=150\n",
      "[DDQN] state=('HC', 'VHD') -> action p_m=0.50, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   7%|█▎                 | 10/150 [43:24<10:04:00, 258.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.020987 diversity=6.4120 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   7%|█▍                  | 11/150 [47:41<9:58:21, 258.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.004035 diversity=11.5944 reward=150\n",
      "[DDQN] state=('HC', 'VHD') -> action p_m=0.50, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   8%|█▌                  | 12/150 [51:59<9:53:56, 258.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.010375 diversity=7.9001 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   9%|█▋                  | 13/150 [56:17<9:49:19, 258.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.019788 diversity=4.4160 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:   9%|█▋                | 14/150 [1:00:36<9:45:54, 258.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.007338 diversity=0.9123 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  10%|█▊                | 15/150 [1:04:54<9:40:57, 258.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.032785 diversity=0.0000 reward=25\n",
      "[DDQN] state=('VHC', 'VLD') -> action p_m=0.25, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  11%|█▉                | 16/150 [1:09:12<9:36:51, 258.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.006785 diversity=2.9766 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  11%|██                | 17/150 [1:13:34<9:34:44, 259.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.002185 diversity=8.0200 reward=100\n",
      "[DDQN] state=('LC', 'VHD') -> action p_m=0.50, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  12%|██▏               | 18/150 [1:17:52<9:30:01, 259.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.047893 diversity=8.1832 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  13%|██▎               | 19/150 [1:22:10<9:24:44, 258.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.029090 diversity=10.4663 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  13%|██▍               | 20/150 [1:26:28<9:19:44, 258.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.040416 diversity=3.1120 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  14%|██▌               | 21/150 [1:30:46<9:15:14, 258.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.004657 diversity=7.4380 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  15%|██▋               | 22/150 [1:35:04<9:10:50, 258.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.054565 diversity=6.0920 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  15%|██▊               | 23/150 [1:39:21<9:05:57, 257.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.021044 diversity=4.2129 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  16%|██▉               | 24/150 [1:43:39<9:01:34, 257.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.043210 diversity=3.1984 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  17%|███               | 25/150 [1:47:57<8:57:32, 258.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.020522 diversity=3.3149 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  17%|███               | 26/150 [1:52:18<8:54:49, 258.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.071569 diversity=5.4876 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  18%|███▏              | 27/150 [1:56:35<8:49:19, 258.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.057337 diversity=5.1037 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  19%|███▎              | 28/150 [2:00:55<8:46:23, 258.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.001325 diversity=5.4390 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  19%|███▍              | 29/150 [2:05:20<8:45:52, 260.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.002331 diversity=0.0000 reward=12.5\n",
      "[DDQN] state=('LC', 'VLD') -> action p_m=0.75, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  20%|███▌              | 30/150 [2:09:44<8:43:09, 261.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.017659 diversity=5.5946 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  21%|███▋              | 31/150 [2:14:20<8:47:27, 265.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.026020 diversity=2.9042 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  21%|███▊              | 32/150 [2:18:44<8:42:13, 265.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.042226 diversity=6.1439 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  22%|███▉              | 33/150 [2:23:12<8:38:57, 266.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.018390 diversity=5.4278 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  23%|████              | 34/150 [2:27:41<8:35:59, 266.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.004715 diversity=5.0573 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  23%|████▏             | 35/150 [2:32:11<8:33:43, 268.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.018720 diversity=3.0537 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  24%|████▎             | 36/150 [2:37:24<8:54:55, 281.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.004926 diversity=2.4600 reward=150\n",
      "[DDQN] state=('HC', 'VHD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  25%|████▍             | 37/150 [2:43:04<9:22:55, 298.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.023966 diversity=1.5236 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  25%|████▌             | 38/150 [2:48:41<9:39:38, 310.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.008041 diversity=3.6772 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  26%|████▋             | 39/150 [2:54:01<9:39:31, 313.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.052559 diversity=4.4333 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  27%|████▊             | 40/150 [2:59:22<9:38:43, 315.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.062194 diversity=6.3613 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  27%|████▉             | 41/150 [3:04:39<9:33:49, 315.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.002621 diversity=4.8066 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  28%|█████             | 42/150 [3:09:42<9:21:56, 312.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.040531 diversity=4.0086 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  29%|█████▏            | 43/150 [3:14:46<9:12:20, 309.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.018620 diversity=4.9256 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  29%|█████▎            | 44/150 [3:19:56<9:07:18, 309.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.046164 diversity=5.0140 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  30%|█████▍            | 45/150 [3:25:03<9:00:36, 308.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.075431 diversity=4.4915 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  31%|█████▌            | 46/150 [3:30:09<8:54:07, 308.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.019636 diversity=4.7322 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  31%|█████▋            | 47/150 [3:35:45<9:02:52, 316.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.037106 diversity=1.9070 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  32%|█████▊            | 48/150 [3:40:31<8:42:37, 307.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.011286 diversity=3.4983 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  33%|█████▉            | 49/150 [3:45:40<8:38:01, 307.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.028507 diversity=2.9739 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=1.00\n",
      "[DDQN] improvement=-0.012472 diversity=4.8915 reward=-100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  33%|██████            | 50/150 [3:50:59<8:38:28, 311.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  34%|██████            | 51/150 [3:55:57<8:26:49, 307.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.053464 diversity=4.5986 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  35%|██████▏           | 52/150 [4:00:54<8:16:36, 304.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.186685 diversity=0.3615 reward=-250\n",
      "[DDQN] state=('Increased', 'LD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  35%|██████▎           | 53/150 [4:05:50<8:07:55, 301.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.071829 diversity=0.5789 reward=100\n",
      "[DDQN] state=('VHC', 'MD') -> action p_m=0.75, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  36%|██████▍           | 54/150 [4:10:51<8:02:36, 301.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.066762 diversity=5.4298 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  37%|██████▌           | 55/150 [4:15:48<7:55:07, 300.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.071317 diversity=0.0000 reward=25\n",
      "[DDQN] state=('VHC', 'VLD') -> action p_m=0.75, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  37%|██████▋           | 56/150 [4:20:44<7:48:30, 299.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.120314 diversity=3.6099 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  38%|██████▊           | 57/150 [4:25:35<7:39:47, 296.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.036380 diversity=4.5501 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  39%|██████▉           | 58/150 [4:30:25<7:31:31, 294.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.042719 diversity=0.0690 reward=25\n",
      "[DDQN] state=('VHC', 'VLD') -> action p_m=0.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  39%|███████           | 59/150 [4:35:16<7:25:08, 293.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.029875 diversity=0.2274 reward=-250\n",
      "[DDQN] state=('Increased', 'LD') -> action p_m=1.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  40%|███████▏          | 60/150 [4:40:14<7:22:06, 294.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.007107 diversity=5.8857 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  41%|███████▎          | 61/150 [4:45:16<7:20:20, 296.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.044918 diversity=0.0000 reward=25\n",
      "[DDQN] state=('VHC', 'VLD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  41%|███████▍          | 62/150 [4:50:15<7:16:22, 297.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.028025 diversity=3.9323 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  42%|███████▌          | 63/150 [4:55:16<7:13:09, 298.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.028720 diversity=7.1017 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  43%|███████▋          | 64/150 [5:00:13<7:07:09, 298.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.068108 diversity=0.4501 reward=100\n",
      "[DDQN] state=('VHC', 'MD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  43%|███████▊          | 65/150 [5:05:18<7:05:22, 300.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.089082 diversity=4.0081 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  44%|███████▉          | 66/150 [5:10:42<7:10:30, 307.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.035224 diversity=4.6928 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  45%|████████          | 67/150 [5:16:45<7:28:17, 324.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.025437 diversity=8.0522 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  45%|████████▏         | 68/150 [5:22:19<7:26:44, 326.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.007867 diversity=0.6185 reward=150\n",
      "[DDQN] state=('VHC', 'HD') -> action p_m=0.25, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  46%|████████▎         | 69/150 [5:27:48<7:22:19, 327.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.009500 diversity=3.5412 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  47%|████████▍         | 70/150 [5:33:01<7:10:48, 323.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.042569 diversity=7.8386 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  47%|████████▌         | 71/150 [5:38:31<7:08:15, 325.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.070196 diversity=3.0541 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  48%|████████▋         | 72/150 [5:43:44<6:57:59, 321.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.009574 diversity=1.3493 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  49%|████████▊         | 73/150 [5:49:16<6:56:38, 324.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.083860 diversity=6.8803 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  49%|████████▉         | 74/150 [5:54:41<6:51:34, 324.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.059172 diversity=4.3223 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  50%|█████████         | 75/150 [6:00:05<6:45:39, 324.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.063118 diversity=2.5215 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  51%|█████████         | 76/150 [6:05:22<6:37:33, 322.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.074704 diversity=1.7053 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  51%|█████████▏        | 77/150 [6:10:36<6:29:14, 319.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.006620 diversity=5.7505 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  52%|█████████▎        | 78/150 [6:15:52<6:22:22, 318.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.078464 diversity=3.9988 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  53%|█████████▍        | 79/150 [6:21:25<6:22:16, 323.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.080844 diversity=0.0000 reward=25\n",
      "[DDQN] state=('VHC', 'VLD') -> action p_m=0.75, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  53%|█████████▌        | 80/150 [6:26:33<6:11:31, 318.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.033823 diversity=3.8944 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  54%|█████████▋        | 81/150 [6:31:43<6:03:10, 315.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.086434 diversity=3.2544 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  55%|█████████▊        | 82/150 [6:36:57<5:57:26, 315.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.058024 diversity=2.2780 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  55%|█████████▉        | 83/150 [6:42:10<5:51:16, 314.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.024825 diversity=3.2001 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  56%|██████████        | 84/150 [6:47:24<5:45:47, 314.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.007808 diversity=2.7199 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  57%|██████████▏       | 85/150 [6:52:40<5:41:12, 314.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.006890 diversity=0.5405 reward=-200\n",
      "[DDQN] state=('Increased', 'MD') -> action p_m=0.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  57%|██████████▎       | 86/150 [6:58:01<5:37:59, 316.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.007657 diversity=1.9987 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  58%|██████████▍       | 87/150 [7:03:16<5:32:10, 316.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.000776 diversity=4.8935 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  59%|██████████▌       | 88/150 [7:08:27<5:25:03, 314.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.003109 diversity=0.0000 reward=-300\n",
      "[DDQN] state=('Increased', 'VLD') -> action p_m=0.75, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  59%|██████████▋       | 89/150 [7:13:42<5:20:04, 314.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.002278 diversity=5.5530 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  60%|██████████▊       | 90/150 [7:18:55<5:14:03, 314.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.014990 diversity=5.4830 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  61%|██████████▉       | 91/150 [7:24:11<5:09:40, 314.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.032732 diversity=2.7656 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  61%|███████████       | 92/150 [7:29:28<5:04:57, 315.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.071684 diversity=4.2657 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  62%|███████████▏      | 93/150 [7:34:47<5:00:45, 316.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.037759 diversity=5.7554 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  63%|███████████▎      | 94/150 [7:40:02<4:54:56, 316.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.019778 diversity=0.7850 reward=150\n",
      "[DDQN] state=('VHC', 'HD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  63%|███████████▍      | 95/150 [7:45:19<4:49:54, 316.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.014236 diversity=5.6570 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  64%|███████████▌      | 96/150 [7:50:34<4:44:14, 315.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.006766 diversity=10.7911 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  65%|███████████▋      | 97/150 [7:55:49<4:38:46, 315.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.003145 diversity=3.4904 reward=150\n",
      "[DDQN] state=('HC', 'VHD') -> action p_m=1.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  65%|███████████▊      | 98/150 [8:01:00<4:32:17, 314.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.081981 diversity=6.2013 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  66%|███████████▉      | 99/150 [8:06:03<4:24:22, 311.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.097450 diversity=5.0800 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  67%|███████████▎     | 100/150 [8:11:14<4:19:07, 310.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.085208 diversity=4.1236 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  67%|███████████▍     | 101/150 [8:16:34<4:16:02, 313.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.064102 diversity=7.6180 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  68%|███████████▌     | 102/150 [8:21:52<4:11:59, 314.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.007716 diversity=3.4670 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  69%|███████████▋     | 103/150 [8:27:11<4:07:35, 316.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.038270 diversity=5.6088 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  69%|███████████▊     | 104/150 [8:32:28<4:02:30, 316.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.008132 diversity=5.9028 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  70%|███████████▉     | 105/150 [8:37:49<3:58:24, 317.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.014894 diversity=5.2201 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  71%|████████████     | 106/150 [8:42:55<3:50:28, 314.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.060945 diversity=3.9183 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  71%|████████████▏    | 107/150 [8:48:18<3:47:04, 316.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.050771 diversity=3.0232 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  72%|████████████▏    | 108/150 [8:53:50<3:44:55, 321.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.014451 diversity=3.0867 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  73%|████████████▎    | 109/150 [8:58:48<3:34:51, 314.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.003737 diversity=4.1710 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  73%|████████████▍    | 110/150 [9:03:48<3:26:41, 310.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.019015 diversity=5.6717 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  74%|████████████▌    | 111/150 [9:08:57<3:21:26, 309.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.046365 diversity=8.2718 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  75%|████████████▋    | 112/150 [9:14:02<3:15:11, 308.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.030116 diversity=3.0868 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  75%|████████████▊    | 113/150 [9:19:17<3:11:21, 310.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.005797 diversity=6.8790 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  76%|████████████▉    | 114/150 [9:24:36<3:07:43, 312.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.005266 diversity=0.0000 reward=-300\n",
      "[DDQN] state=('Increased', 'VLD') -> action p_m=0.25, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  77%|█████████████    | 115/150 [9:29:52<3:03:09, 313.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.046113 diversity=3.9256 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  77%|█████████████▏   | 116/150 [9:34:54<2:55:49, 310.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.057173 diversity=3.8839 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  78%|█████████████▎   | 117/150 [9:40:07<2:51:07, 311.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.072005 diversity=5.9503 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  79%|█████████████▎   | 118/150 [9:45:32<2:48:08, 315.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.002342 diversity=4.2456 reward=100\n",
      "[DDQN] state=('LC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  79%|█████████████▍   | 119/150 [9:51:02<2:45:13, 319.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.000210 diversity=4.2666 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  80%|█████████████▌   | 120/150 [9:56:25<2:40:21, 320.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.018415 diversity=9.0453 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  81%|████████████▉   | 121/150 [10:01:59<2:36:58, 324.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.010296 diversity=6.3445 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  81%|█████████████   | 122/150 [10:07:33<2:32:46, 327.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.040226 diversity=6.2452 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  82%|█████████████   | 123/150 [10:13:08<2:28:22, 329.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.035582 diversity=5.2573 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  83%|█████████████▏  | 124/150 [10:19:09<2:26:55, 339.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.006486 diversity=3.6166 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  83%|█████████████▎  | 125/150 [10:25:41<2:27:56, 355.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.026526 diversity=5.3663 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  84%|█████████████▍  | 126/150 [10:32:10<2:26:04, 365.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.016764 diversity=3.8772 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  85%|█████████████▌  | 127/150 [10:38:33<2:22:00, 370.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.025463 diversity=4.4043 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  85%|█████████████▋  | 128/150 [10:44:51<2:16:38, 372.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.007171 diversity=3.5170 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  86%|█████████████▊  | 129/150 [10:51:07<2:10:51, 373.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.080116 diversity=4.6768 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.50, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  87%|█████████████▊  | 130/150 [10:57:53<2:07:47, 383.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.029096 diversity=4.2084 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.25, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  87%|█████████████▉  | 131/150 [11:04:30<2:02:45, 387.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.065689 diversity=2.6803 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  88%|██████████████  | 132/150 [11:11:01<1:56:32, 388.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.012824 diversity=4.8588 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.75, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  89%|██████████████▏ | 133/150 [11:17:26<1:49:49, 387.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.018287 diversity=6.7576 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  89%|██████████████▎ | 134/150 [11:23:54<1:43:20, 387.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.027889 diversity=2.7358 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.75, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  90%|██████████████▍ | 135/150 [11:30:35<1:37:53, 391.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.003066 diversity=5.1281 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  91%|██████████████▌ | 136/150 [11:37:09<1:31:34, 392.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.015820 diversity=4.9814 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  91%|██████████████▌ | 137/150 [11:43:51<1:25:38, 395.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.000957 diversity=2.7724 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  92%|██████████████▋ | 138/150 [11:50:38<1:19:46, 398.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.039517 diversity=4.6188 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  93%|██████████████▊ | 139/150 [11:57:34<1:14:02, 403.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.015862 diversity=3.5896 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=1.00, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  93%|██████████████▉ | 140/150 [12:04:54<1:09:08, 414.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.025610 diversity=7.1168 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  94%|███████████████ | 141/150 [12:11:52<1:02:20, 415.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.000514 diversity=0.0000 reward=-300\n",
      "[DDQN] state=('Increased', 'VLD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  95%|█████████████████ | 142/150 [12:18:28<54:37, 409.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.044051 diversity=4.0247 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  95%|█████████████████▏| 143/150 [12:23:24<43:49, 375.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.044314 diversity=3.2257 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  96%|█████████████████▎| 144/150 [12:28:20<35:09, 351.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.000455 diversity=6.7275 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.00, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  97%|█████████████████▍| 145/150 [12:33:28<28:12, 338.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.000372 diversity=0.7404 reward=-150\n",
      "[DDQN] state=('Increased', 'HD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  97%|█████████████████▌| 146/150 [12:38:25<21:45, 326.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.007954 diversity=4.7226 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  98%|█████████████████▋| 147/150 [12:43:27<15:57, 319.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.011491 diversity=3.7188 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  99%|█████████████████▊| 148/150 [12:48:27<10:26, 313.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=-0.014949 diversity=3.2645 reward=-100\n",
      "[DDQN] state=('Increased', 'VHD') -> action p_m=0.50, p_c=0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN:  99%|█████████████████▉| 149/150 [12:53:27<05:09, 309.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.006108 diversity=5.2442 reward=200\n",
      "[DDQN] state=('VHC', 'VHD') -> action p_m=0.25, p_c=0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DDQN: 100%|██████████████████| 150/150 [12:58:27<00:00, 311.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDQN] improvement=0.021031 diversity=4.3303 reward=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[FCM] iter=0\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.00, p_c=0.00\n",
      "[DDQN] improvement=0.000000 diversity=0.0000 reward=-30\n",
      "[FCM] J=5.830140  Δ=inf  bw[0..3]=[7.5336202  6.62755648 0.98122294]\n",
      "\n",
      "[FCM] iter=1\n",
      "[DDQN] state=('Stalled', 'VLD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=4.9889 reward=0\n",
      "[FCM] J=0.286692  Δ=5.543e+00  bw[0..3]=[ 7.23128742 16.90996263  1.        ]\n",
      "\n",
      "[FCM] iter=2\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=5.2896 reward=0\n",
      "[FCM] J=0.172898  Δ=1.138e-01  bw[0..3]=[ 6.57894944 24.19118623  1.        ]\n",
      "\n",
      "[FCM] iter=3\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=5.4057 reward=0\n",
      "[FCM] J=0.140804  Δ=3.209e-02  bw[0..3]=[ 9.79147604 18.82859232  1.        ]\n",
      "\n",
      "[FCM] iter=4\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.25, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=3.0969 reward=0\n",
      "[FCM] J=0.150307  Δ=9.503e-03  bw[0..3]=[14.77584023 16.70889423  1.        ]\n",
      "\n",
      "[FCM] iter=5\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=3.1515 reward=0\n",
      "[FCM] J=2.963447  Δ=2.813e+00  bw[0..3]=[8.15579635 7.1498389  1.        ]\n",
      "\n",
      "[FCM] iter=6\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=3.2151 reward=0\n",
      "[FCM] J=1.017820  Δ=1.946e+00  bw[0..3]=[ 6.17530647 11.34585165  1.        ]\n",
      "\n",
      "[FCM] iter=7\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.75\n",
      "[DDQN] improvement=0.000000 diversity=4.1843 reward=0\n",
      "[FCM] J=4.609727  Δ=3.592e+00  bw[0..3]=[2.93516836 9.47024722 1.        ]\n",
      "\n",
      "[FCM] iter=8\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=4.0481 reward=0\n",
      "[FCM] J=0.650138  Δ=3.960e+00  bw[0..3]=[ 8.4626859  11.83479999  1.        ]\n",
      "\n",
      "[FCM] iter=9\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=3.7253 reward=0\n",
      "[FCM] J=0.183798  Δ=4.663e-01  bw[0..3]=[ 9.51616961 17.39217068  1.        ]\n",
      "\n",
      "[FCM] iter=10\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.75\n",
      "[DDQN] improvement=0.000000 diversity=5.9105 reward=0\n",
      "[FCM] J=0.416301  Δ=2.325e-01  bw[0..3]=[ 8.55707409 13.63473986  1.        ]\n",
      "\n",
      "[FCM] iter=11\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.00, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=0.4077 reward=-10\n",
      "[FCM] J=0.412772  Δ=3.529e-03  bw[0..3]=[ 8.6542635  13.63473986  1.        ]\n",
      "\n",
      "[FCM] iter=12\n",
      "[DDQN] state=('Stalled', 'MD') -> action p_m=0.50, p_c=1.00\n",
      "[DDQN] improvement=0.000000 diversity=4.5125 reward=0\n",
      "[FCM] J=0.173373  Δ=2.394e-01  bw[0..3]=[ 6.26370007 27.89125722  1.        ]\n",
      "\n",
      "[FCM] iter=13\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.75, p_c=0.25\n",
      "[DDQN] improvement=0.000000 diversity=4.1598 reward=0\n",
      "[FCM] J=0.171759  Δ=1.614e-03  bw[0..3]=[ 6.26370007 27.89125722  1.        ]\n",
      "\n",
      "[FCM] iter=14\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.25, p_c=0.75\n",
      "[DDQN] improvement=0.000000 diversity=1.9432 reward=0\n",
      "[FCM] J=7.725139  Δ=7.553e+00  bw[0..3]=[3.15520783 6.64748918 1.        ]\n",
      "\n",
      "[FCM] iter=15\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=1.00, p_c=0.00\n",
      "[DDQN] improvement=0.000000 diversity=6.2833 reward=0\n",
      "[FCM] J=0.362657  Δ=7.362e+00  bw[0..3]=[10.94388304 13.49970206  1.        ]\n",
      "\n",
      "[FCM] iter=16\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=5.6338 reward=0\n",
      "[FCM] J=8.626512  Δ=8.264e+00  bw[0..3]=[ 2.10265633 13.21699779  1.        ]\n",
      "\n",
      "[FCM] iter=17\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.25, p_c=0.00\n",
      "[DDQN] improvement=0.000000 diversity=2.7886 reward=0\n",
      "[FCM] J=0.200595  Δ=8.426e+00  bw[0..3]=[ 7.80131905 18.33046421  1.        ]\n",
      "\n",
      "[FCM] iter=18\n",
      "[DDQN] state=('Stalled', 'VHD') -> action p_m=0.50, p_c=0.50\n",
      "[DDQN] improvement=0.000000 diversity=4.3745 reward=0\n",
      "[FCM] J=0.200003  Δ=5.920e-04  bw[0..3]=[16.3861294  15.07510173  1.        ]\n",
      "[FCM] Converged at iter 18\n",
      "[Agent] Saved to ./checkpoints/ddqn_agent.pt\n",
      "\n",
      "=== RESULTS (Your CSVs) ===\n",
      "FARI(U_final, true_post) = 0.758340\n",
      "FARI(U_final, init_U)    = 0.672074\n",
      "Final bandwidth (all): [16.3861294  15.07510173  1.        ]\n",
      "Saved: ./outputs/U_final.csv, ./outputs/V_final.csv, ./outputs/bw_final.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# rl_ga_kdss_fcm_gpu.py\n",
    "# DDQN + GA bandwidth tuning for KDSS-based FCM on mixed-type data (no R/Colab), GPU/MPS-aware.\n",
    "# Requires: numpy, pandas, torch, matplotlib, tqdm, scipy\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "from collections import deque\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------\n",
    "# PyTorch (for DDQN)\n",
    "# -------------------------\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "except Exception as e:\n",
    "    print(\"ERROR: This script needs PyTorch (torch) for the DDQN agent.\\n\"\n",
    "          \"Install with: pip install torch --index-url https://download.pytorch.org/whl/cpu\", file=sys.stderr)\n",
    "    raise\n",
    "\n",
    "# -------------------------\n",
    "# Device picker (CUDA / MPS / CPU)\n",
    "# -------------------------\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# -------------------------\n",
    "# Utilities\n",
    "# -------------------------\n",
    "def ensure_dir(path: str):\n",
    "    if path and not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def parse_index_list(s):\n",
    "    \"\"\"Parse comma-separated indices like '0,2,5'. Return list[int] or None if empty/None.\"\"\"\n",
    "    if s is None or str(s).strip()==\"\":\n",
    "        return None\n",
    "    return [int(x.strip()) for x in s.split(\",\") if x.strip()!=\"\"]\n",
    "\n",
    "def infer_column_types(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Heuristic:\n",
    "      - float columns -> continuous\n",
    "      - int columns with <= 12 unique values -> ordinal\n",
    "      - object/category -> nominal\n",
    "    \"\"\"\n",
    "    con_idx, fac_idx, ord_idx = [], [], []\n",
    "    for j, col in enumerate(df.columns):\n",
    "        s = df[col]\n",
    "        if pd.api.types.is_float_dtype(s):\n",
    "            con_idx.append(j)\n",
    "        elif pd.api.types.is_integer_dtype(s):\n",
    "            nunq = s.nunique(dropna=True)\n",
    "            if nunq <= 12:\n",
    "                ord_idx.append(j)\n",
    "            else:\n",
    "                con_idx.append(j)  # high-cardinality ints treated as continuous\n",
    "        else:\n",
    "            fac_idx.append(j)\n",
    "    return con_idx, fac_idx, ord_idx\n",
    "\n",
    "def encode_mixed_df(df: pd.DataFrame, fac_idx=None, ord_idx=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Encode a mixed-type DataFrame into purely numeric columns:\n",
    "\n",
    "      - Continuous columns: left as-is (but cast to numeric).\n",
    "      - fac_idx (nominal): converted to unordered categorical codes.\n",
    "      - ord_idx (ordinal): converted to ordered categorical codes (numeric sort if possible, else lexical), then to codes.\n",
    "      - Any remaining non-numeric columns: treated as nominal and coded.\n",
    "\n",
    "    Returns a numeric DataFrame (ints/floats) suitable for distance computations.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    n_cols = df2.shape[1]\n",
    "\n",
    "    fac_idx = [] if fac_idx is None else list(fac_idx)\n",
    "    ord_idx = [] if ord_idx is None else list(ord_idx)\n",
    "\n",
    "    # 1) Handle nominal (unordered categorical)\n",
    "    for j in fac_idx:\n",
    "        s = df2.iloc[:, j]\n",
    "        if not isinstance(s.dtype, pd.CategoricalDtype):\n",
    "            s = s.astype(\"category\")\n",
    "        s = s.cat.as_unordered()\n",
    "        df2.iloc[:, j] = s.cat.codes.astype(\"int64\")\n",
    "\n",
    "    # 2) Handle ordinal (ordered categorical)\n",
    "    for j in ord_idx:\n",
    "        s = df2.iloc[:, j]\n",
    "        if isinstance(s.dtype, pd.CategoricalDtype):\n",
    "            s = s.cat.as_ordered()\n",
    "            df2.iloc[:, j] = pd.Series(s).cat.codes.astype(\"int64\")\n",
    "        else:\n",
    "            # Try numeric ordering first\n",
    "            try:\n",
    "                vals = pd.to_numeric(s, errors=\"raise\")\n",
    "                cats = np.unique(vals.dropna())\n",
    "                # Use string categories to be robust to mixed types while preserving order\n",
    "                cat_strings = [str(c) for c in np.sort(cats)]\n",
    "                s_str = s.astype(str)\n",
    "                s_ord = pd.Categorical(s_str, categories=cat_strings, ordered=True)\n",
    "                df2.iloc[:, j] = pd.Series(s_ord).cat.codes.astype(\"int64\")\n",
    "            except Exception:\n",
    "                # Fallback to lexical ordering\n",
    "                cats = sorted(s.astype(str).dropna().unique())\n",
    "                s_ord = pd.Categorical(s.astype(str), categories=cats, ordered=True)\n",
    "                df2.iloc[:, j] = pd.Series(s_ord).cat.codes.astype(\"int64\")\n",
    "\n",
    "    # 3) Remaining columns: if still non-numeric, treat as nominal codes\n",
    "    for j in range(n_cols):\n",
    "        s = df2.iloc[:, j]\n",
    "        if not pd.api.types.is_numeric_dtype(s):\n",
    "            s = s.astype(\"category\")\n",
    "            df2.iloc[:, j] = s.cat.codes.astype(\"int64\")\n",
    "\n",
    "    # 4) Ensure numeric dtype across the board\n",
    "    for col in df2.columns:\n",
    "        df2[col] = pd.to_numeric(df2[col], errors=\"coerce\")\n",
    "\n",
    "    return df2\n",
    "\n",
    "# -------------------------\n",
    "# FARI (Adjusted Fuzzy Rand Index)\n",
    "# -------------------------\n",
    "def fari(a: np.ndarray, b: np.ndarray):\n",
    "    \"\"\"\n",
    "    Adjusted Fuzzy Rand Index between membership matrices a and b (n x k).\n",
    "    Returns a single float.\n",
    "    \"\"\"\n",
    "    n = a.shape[0]\n",
    "    A = a @ a.T\n",
    "    B = b @ b.T\n",
    "    j = np.ones((n, n))\n",
    "\n",
    "    Na = (np.sum(A) / np.sum(A * A)) * A\n",
    "    Nb = (np.sum(B) / np.sum(B * B)) * B\n",
    "\n",
    "    ri = (np.sum(Na * Nb) + np.sum((j - Na) * (j - Nb)) - n) / (n * (n - 1))\n",
    "\n",
    "    M = j / n\n",
    "    R = np.eye(n) - M\n",
    "\n",
    "    term1 = (2 * np.sum(A) * np.sum(B)) / (np.sum(A * A) * np.sum(B * B))\n",
    "    term2 = np.sum(M * A) * np.sum(M * B) + (1 / (n - 1)) * np.sum(R * A) * np.sum(R * B)\n",
    "    term3 = (np.sum(A) ** 2) / np.sum(A * A)\n",
    "    term4 = (np.sum(B) ** 2) / np.sum(B * B)\n",
    "    Eri = (term1 * term2 - term3 - term4 + n**2 - n) / (n * (n - 1))\n",
    "\n",
    "    ari = (ri - Eri) / (1 - Eri)\n",
    "    return float(ari)\n",
    "\n",
    "# -------------------------\n",
    "# Kernels (continuous / nominal / ordinal)\n",
    "# -------------------------\n",
    "def c_gaussian(A, B, bws, Cmatrix_unused):\n",
    "    return np.sum((2 * np.pi) ** (-0.5) * np.exp(-(((A - B) / bws) ** 2) / 2))\n",
    "\n",
    "def c_uniform(A, B, bws, Cmatrix_unused):\n",
    "    return np.sum(np.where(np.abs((A - B) / bws) <= 1, 0.5, 0.0))\n",
    "\n",
    "def c_epanechnikov(A, B, bws, Cmatrix_unused):\n",
    "    return np.sum(np.where(np.abs((A - B) / bws) <= 1, 0.75 * (1 - ((A - B) / bws) ** 2), 0.0))\n",
    "\n",
    "def c_triangle(A, B, bws, Cmatrix_unused):\n",
    "    return np.sum(np.where(np.abs((A - B) / bws) <= 1, 1 - np.abs((A - B) / bws), 0.0))\n",
    "\n",
    "def c_biweight(A, B, bws, Cmatrix_unused):\n",
    "    z2 = ((A - B) / bws) ** 2\n",
    "    return np.sum(np.where(np.abs((A - B) / bws) <= 1, (15/32.0) * (3 - z2) ** 2, 0.0))\n",
    "\n",
    "def c_triweight(A, B, bws, Cmatrix_unused):\n",
    "    return np.sum(np.where(np.abs((A - B) / bws) <= 1, (35/32.0) * (1 - ((A - B) / bws) ** 2) ** 3, 0.0))\n",
    "\n",
    "def c_tricube(A, B, bws, Cmatrix_unused):\n",
    "    z = np.abs((A - B) / bws)\n",
    "    return np.sum(np.where(z <= 1, (70/81.0) * (1 - z ** 3) ** 3, 0.0))\n",
    "\n",
    "def c_cosine(A, B, bws, Cmatrix_unused):\n",
    "    z = (A - B) / bws\n",
    "    return np.sum(np.where(np.abs(z) <= 1, (np.pi/4.0) * np.cos((np.pi/2.0) * z), 0.0))\n",
    "\n",
    "def c_logistic(A, B, bws, Cmatrix_unused):\n",
    "    z = (A - B) / bws\n",
    "    return np.sum(1.0 / (np.exp(z) + 2.0 + np.exp(-z)))\n",
    "\n",
    "def c_sigmoid(A, B, bws, Cmatrix_unused):\n",
    "    z = (A - B) / bws\n",
    "    return np.sum(2.0 / (np.pi * (np.exp(z) + np.exp(-z))))\n",
    "\n",
    "def c_silverman(A, B, bws, Cmatrix_unused):\n",
    "    z = np.abs((A - B) / bws) / np.sqrt(2.0)\n",
    "    return np.sum((1.0 / bws) * 0.5 * np.exp(-z) * np.sin(z + np.pi / 4.0))\n",
    "\n",
    "# Unordered categorical kernels\n",
    "def u_aitchisonaitken(A, B, bws, Cfull):\n",
    "    res = 0.0\n",
    "    m = len(A)\n",
    "    for j in range(m):\n",
    "        unique_count = len(np.unique(Cfull[:, j]))\n",
    "        if unique_count <= 1:\n",
    "            res += (1 - bws[j])\n",
    "        else:\n",
    "            res += (1 - bws[j]) if A[j] == B[j] else bws[j] / (unique_count - 1)\n",
    "    return res\n",
    "\n",
    "def u_aitken(A, B, bws, Cfull):\n",
    "    res = 0.0\n",
    "    m = len(A)\n",
    "    for j in range(m):\n",
    "        res += 1.0 if A[j] == B[j] else bws[j]\n",
    "    return res\n",
    "\n",
    "# Ordinal kernels\n",
    "def o_wangvanryzin(A, B, bws, Cfull):\n",
    "    if np.all(A == B):\n",
    "        return float(np.sum(1 - bws))\n",
    "    else:\n",
    "        return float(np.sum(0.5 * (1 - bws) * (bws ** np.abs(A - B))))\n",
    "\n",
    "def o_aitchisonaitken(A, B, bws, Cfull):\n",
    "    return float(np.sum(np.where(A == B, 1.0, bws ** np.abs(A - B))))\n",
    "\n",
    "def o_aitken(A, B, bws, Cfull):\n",
    "    return float(np.sum(np.where(A == B, bws, (1 - bws) / (2 ** np.abs(A - B)))))\n",
    "\n",
    "def o_habbema(A, B, bws, Cfull):\n",
    "    return float(np.sum(bws ** (np.abs(A - B) ** 2)))\n",
    "\n",
    "def o_liracine(A, B, bws, Cfull):\n",
    "    return float(np.sum(np.where(A == B, 1.0, bws ** np.abs(A - B))))\n",
    "\n",
    "# -------------------------\n",
    "# KDSS distance (row vs center)\n",
    "# -------------------------\n",
    "def _select_kernel(_unused=None):\n",
    "    ck = {\n",
    "        \"c_gaussian\": c_gaussian, \"c_uniform\": c_uniform, \"c_epanechnikov\": c_epanechnikov,\n",
    "        \"c_triangle\": c_triangle, \"c_biweight\": c_biweight, \"c_triweight\": c_triweight,\n",
    "        \"c_tricube\": c_tricube, \"c_cosine\": c_cosine, \"c_logistic\": c_logistic,\n",
    "        \"c_sigmoid\": c_sigmoid, \"c_silverman\": c_silverman\n",
    "    }\n",
    "    uk = {\"u_aitken\": u_aitken, \"u_aitchisonaitken\": u_aitchisonaitken}\n",
    "    ok = {\"o_wangvanryzin\": o_wangvanryzin, \"o_habbema\": o_habbema,\n",
    "          \"o_aitken\": o_aitken, \"o_aitchisonaitken\": o_aitchisonaitken, \"o_liracine\": o_liracine}\n",
    "    return ck, uk, ok\n",
    "\n",
    "def dkss_distance(x, c, bw,\n",
    "                  cFUN=\"c_gaussian\", uFUN=\"u_aitken\", oFUN=\"o_wangvanryzin\",\n",
    "                  con_idx=None, fac_idx=None, ord_idx=None):\n",
    "    x = np.asarray(x); c = np.asarray(c); bw = np.asarray(bw)\n",
    "    assert x.shape == c.shape == bw.shape, \"x, c, bw must be same length\"\n",
    "\n",
    "    if con_idx is None: con_idx = list(range(len(x)))\n",
    "    if fac_idx is None: fac_idx = []\n",
    "    if ord_idx is None: ord_idx = []\n",
    "\n",
    "    ck, uk, ok = _select_kernel()\n",
    "    c_fun = ck[cFUN]; u_fun = uk[uFUN]; o_fun = ok[oFUN]\n",
    "\n",
    "    def compute(FUN, idx):\n",
    "        if len(idx) == 0: return 0.0\n",
    "        a = x[idx]; b = c[idx]; bws = bw[idx]\n",
    "        full = np.vstack([x, c])[:, idx]  # small \"full\" matrix (2 x |idx|)\n",
    "        return FUN(a, a, bws, full) + FUN(b, b, bws, full) - FUN(a, b, bws, full) - FUN(b, a, bws, full)\n",
    "\n",
    "    return float(compute(c_fun, con_idx) + compute(u_fun, fac_idx) + compute(o_fun, ord_idx))\n",
    "\n",
    "# -------------------------\n",
    "# FCM inner loop (no bw update) used inside GA objective\n",
    "# -------------------------\n",
    "def kdml_fcm_no_bw_update(X, C, bw, m, epsilon, con_idx, fac_idx, ord_idx, max_iter=1):\n",
    "    N, D = X.shape\n",
    "    U = np.random.rand(N, C)\n",
    "    U /= U.sum(axis=1, keepdims=True)\n",
    "\n",
    "    J_prev = float('inf')\n",
    "    for _ in range(max_iter):\n",
    "        # update centers (simple weighted mean on encoded features)\n",
    "        V = np.zeros((C, D))\n",
    "        for c in range(C):\n",
    "            w = (U[:, c] ** m).reshape(-1, 1)\n",
    "            num = np.sum(w * X, axis=0)\n",
    "            den = np.sum(U[:, c] ** m)\n",
    "            V[c] = num / max(den, 1e-12)\n",
    "\n",
    "        # distances\n",
    "        Dmat = np.zeros((N, C))\n",
    "        for i in range(N):\n",
    "            for c in range(C):\n",
    "                Dmat[i, c] = dkss_distance(\n",
    "                    X[i], V[c], bw=bw,\n",
    "                    cFUN=\"c_gaussian\", uFUN=\"u_aitken\", oFUN=\"o_wangvanryzin\",\n",
    "                    con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx\n",
    "                )\n",
    "\n",
    "        # update U\n",
    "        for i in range(N):\n",
    "            for c in range(C):\n",
    "                if Dmat[i, c] == 0:\n",
    "                    U[i, :] = 0; U[i, c] = 1\n",
    "                else:\n",
    "                    denom = 0.0\n",
    "                    for j in range(C):\n",
    "                        denom += (Dmat[i, c] / (Dmat[i, j] if Dmat[i, j] > 0 else 1e-12)) ** (2.0 / (m - 1))\n",
    "                    U[i, c] = 1.0 / max(denom, 1e-12)\n",
    "\n",
    "        J = np.sum((U ** m) * (Dmat ** 2))\n",
    "        if abs(J - J_prev) < epsilon: break\n",
    "        J_prev = J\n",
    "\n",
    "    return V, U\n",
    "\n",
    "# -------------------------\n",
    "# GA objective (negated FARI vs init membership)\n",
    "# -------------------------\n",
    "def objective(bw_candidate, U_ref, X, C, m, con_idx, fac_idx, ord_idx, epsilon=0.01, max_iter_obj=1):\n",
    "    _, U = kdml_fcm_no_bw_update(X=X, C=C, bw=np.asarray(bw_candidate), m=m, epsilon=epsilon,\n",
    "                                 con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx, max_iter=max_iter_obj)\n",
    "    return -fari(U, U_ref)\n",
    "\n",
    "# -------------------------\n",
    "# GA components\n",
    "# -------------------------\n",
    "def compute_diversity(population):\n",
    "    arr = np.array(population)\n",
    "    return float(np.mean(np.std(arr, axis=0)))\n",
    "\n",
    "def clamp_continuous(candidate, con_idx):\n",
    "    cand = list(candidate)\n",
    "    if con_idx is None:\n",
    "        for i in range(len(cand)):\n",
    "            cand[i] = max(cand[i], 0.1)\n",
    "    else:\n",
    "        for i in con_idx:\n",
    "            cand[i] = max(cand[i], 0.1)\n",
    "    return tuple(cand)\n",
    "\n",
    "def tournament_selection(pop, tournament_size, U_ref, X, C, m, con_idx, fac_idx, ord_idx, epsilon=0.01, max_iter_obj=1):\n",
    "    t = random.sample(pop, tournament_size)\n",
    "    return min(t, key=lambda ind: objective(ind, U_ref=U_ref, X=X, C=C, m=m,\n",
    "                                            con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx,\n",
    "                                            epsilon=epsilon, max_iter_obj=max_iter_obj))\n",
    "\n",
    "def one_point_crossover(p1, p2):\n",
    "    point = random.randint(1, len(p1) - 1)\n",
    "    return p1[:point] + p2[point:], p2[:point] + p1[point:]\n",
    "\n",
    "def mutate(individual, p_m, sigma, con_idx, fac_idx, ord_idx):\n",
    "    out = []\n",
    "    for i, gene in enumerate(individual):\n",
    "        g = gene\n",
    "        if random.random() < p_m:\n",
    "            g = g + np.random.normal(0, sigma)\n",
    "            if (fac_idx and i in fac_idx) or (ord_idx and i in ord_idx):\n",
    "                g = max(0.0, min(g, 1.0))\n",
    "            else:\n",
    "                g = max(g, 0.1)\n",
    "        out.append(g)\n",
    "    return tuple(out)\n",
    "\n",
    "def genetic_algorithm(pop_size, num_generations, bandwidth_length, p_m, p_c, tournament_size, sigma,\n",
    "                      U_ref, X, C, m, con_idx, fac_idx, ord_idx,\n",
    "                      verbose=False, initial_bw=None, max_iter_obj=1):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        cand = []\n",
    "        for i in range(bandwidth_length):\n",
    "            if (fac_idx and i in fac_idx) or (ord_idx and i in ord_idx):\n",
    "                cand.append(random.uniform(0.0, 1.0))\n",
    "            else:\n",
    "                cand.append(random.uniform(0.001, 10.0))\n",
    "        population.append(clamp_continuous(tuple(cand), con_idx))\n",
    "    if initial_bw is not None:\n",
    "        population[0] = clamp_continuous(tuple(initial_bw), con_idx)\n",
    "\n",
    "    best = None\n",
    "    best_fit = float('inf')\n",
    "    prev_gen_best = float('inf')\n",
    "\n",
    "    for gen in range(num_generations):\n",
    "        gen_best_fit = float('inf'); gen_best = None\n",
    "        fits = []\n",
    "        for ind in population:\n",
    "            f = objective(ind, U_ref=U_ref, X=X, C=C, m=m,\n",
    "                          con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx,\n",
    "                          max_iter_obj=max_iter_obj)\n",
    "            fits.append(f)\n",
    "            if f < gen_best_fit:\n",
    "                gen_best_fit, gen_best = f, ind\n",
    "            if f < best_fit:\n",
    "                best_fit, best = f, ind\n",
    "\n",
    "        if verbose:\n",
    "            div = compute_diversity(population)\n",
    "            imp = 0 if prev_gen_best==float('inf') else (prev_gen_best - gen_best_fit)\n",
    "            print(f\"[GA] gen={gen+1}/{num_generations} best={gen_best_fit:.4f} avg={np.mean(fits):.4f} div={div:.4f} imp={imp:.4f}\")\n",
    "        prev_gen_best = gen_best_fit\n",
    "\n",
    "        new_pop = []\n",
    "        while len(new_pop) < pop_size:\n",
    "            p1 = tournament_selection(population, tournament_size, U_ref, X, C, m, con_idx, fac_idx, ord_idx, max_iter_obj=max_iter_obj)\n",
    "            p2 = tournament_selection(population, tournament_size, U_ref, X, C, m, con_idx, fac_idx, ord_idx, max_iter_obj=max_iter_obj)\n",
    "            if random.random() < p_c:\n",
    "                c1, c2 = one_point_crossover(p1, p2)\n",
    "            else:\n",
    "                c1, c2 = p1, p2\n",
    "            c1 = mutate(c1, p_m, sigma, con_idx, fac_idx, ord_idx)\n",
    "            c2 = mutate(c2, p_m, sigma, con_idx, fac_idx, ord_idx)\n",
    "            new_pop.extend([clamp_continuous(c1, con_idx), clamp_continuous(c2, con_idx)])\n",
    "        population = new_pop[:pop_size]\n",
    "\n",
    "    return best, best_fit, population\n",
    "\n",
    "# -------------------------\n",
    "# DDQN agent (device-aware)\n",
    "# -------------------------\n",
    "VHC=\"VHC\"; HC=\"HC\"; LC=\"LC\"; STALLED=\"Stalled\"; INCREASED=\"Increased\"\n",
    "VHD=\"VHD\"; HD=\"HD\"; MD=\"MD\"; LD=\"LD\"; VLD=\"VLD\"\n",
    "\n",
    "REWARD_TABLE = {\n",
    "    (VHC, VHD): 200,   (VHC, HD): 150,   (VHC, MD): 100,   (VHC, LD): 50,    (VHC, VLD): 25,\n",
    "    (HC,  VHD): 150,   (HC,  HD): 112.5, (HC,  MD): 75,    (HC,  LD): 37.5,  (HC,  VLD): 18.75,\n",
    "    (LC,  VHD): 100,   (LC,  HD): 75,    (LC,  MD): 50,    (LC,  LD): 25,    (LC,  VLD): 12.5,\n",
    "    (STALLED, VHD): 0, (STALLED, HD): 0, (STALLED, MD): -10, (STALLED, LD): -20, (STALLED, VLD): -30,\n",
    "    (INCREASED, VHD): -100, (INCREASED, HD): -150, (INCREASED, MD): -200, (INCREASED, LD): -250, (INCREASED, VLD): -300\n",
    "}\n",
    "\n",
    "def discretize_improvement(improvement):\n",
    "    if improvement > 0.005: return VHC\n",
    "    if improvement > 0.003: return HC\n",
    "    if improvement > 0.001: return LC\n",
    "    if improvement == 0:    return STALLED\n",
    "    return INCREASED\n",
    "\n",
    "def discretize_diversity(diversity):\n",
    "    if diversity > 0.8: return VHD\n",
    "    if diversity > 0.6: return HD\n",
    "    if diversity > 0.4: return MD\n",
    "    if diversity > 0.2: return LD\n",
    "    return VLD\n",
    "\n",
    "def state_to_index(state):\n",
    "    imp_map = {VHC:0, HC:1, LC:2, STALLED:3, INCREASED:4}\n",
    "    div_map = {VHD:0, HD:1, MD:2, LD:3, VLD:4}\n",
    "    return imp_map[state[0]]*5 + div_map[state[1]]\n",
    "\n",
    "def onehot_from_index(idx, size=25):\n",
    "    v = np.zeros(size, dtype=np.float32); v[idx]=1; return v\n",
    "\n",
    "def index_to_action(idx):\n",
    "    p_m = (idx // 5)/4.0\n",
    "    p_c = (idx % 5)/4.0\n",
    "    return (p_m, p_c)\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dims, out_dim):\n",
    "        super().__init__()\n",
    "        layers=[]; prev=in_dim\n",
    "        for h in hidden_dims:\n",
    "            layers += [nn.Linear(prev, h), nn.ReLU()]\n",
    "            prev = h\n",
    "        layers += [nn.Linear(prev, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_size=25, action_size=25, hidden_dims=[64,64],\n",
    "                 lr=1e-3, gamma=0.9, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995,\n",
    "                 buffer_size=10000, batch_size=32, target_update_freq=100, device=None):\n",
    "        self.state_size=state_size; self.action_size=action_size\n",
    "        self.gamma=gamma\n",
    "        self.epsilon=epsilon; self.epsilon_min=epsilon_min; self.epsilon_decay=epsilon_decay\n",
    "        self.batch_size=batch_size; self.target_update_freq=target_update_freq\n",
    "        self.update_counter=0\n",
    "        self.device = device or pick_device()\n",
    "\n",
    "        self.online_net = QNetwork(state_size, hidden_dims, action_size).to(self.device)\n",
    "        self.target_net = QNetwork(state_size, hidden_dims, action_size).to(self.device)\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict()); self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.online_net.parameters(), lr=lr)\n",
    "        self.replay = deque(maxlen=buffer_size)\n",
    "\n",
    "        # Debug: confirm model device\n",
    "        print(f\"[DDQN] Using device: {self.device} | param device: {next(self.online_net.parameters()).device}\")\n",
    "\n",
    "    def get_state(self, improvement, diversity):\n",
    "        return (discretize_improvement(improvement), discretize_diversity(diversity))\n",
    "\n",
    "    def _encode(self, state):\n",
    "        return onehot_from_index(state_to_index(state), self.state_size)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size-1)\n",
    "        s = torch.from_numpy(self._encode(state)).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q = self.online_net(s)\n",
    "        return int(q.argmax(dim=1))\n",
    "\n",
    "    def store(self, s,a,r,s2,done):\n",
    "        self.replay.append((s,a,r,s2,done))\n",
    "\n",
    "    def update_network(self):\n",
    "        if len(self.replay) < self.batch_size: return\n",
    "        batch = random.sample(self.replay, self.batch_size)\n",
    "        S, A, R, S2, D = zip(*batch)\n",
    "        S  = torch.from_numpy(np.vstack([self._encode(s)  for s  in S ])).to(self.device)\n",
    "        S2 = torch.from_numpy(np.vstack([self._encode(s2) for s2 in S2])).to(self.device)\n",
    "        A  = torch.tensor(A, dtype=torch.long, device=self.device).unsqueeze(1)\n",
    "        R  = torch.tensor(R, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "        D  = torch.tensor(D, dtype=torch.float32, device=self.device).unsqueeze(1)\n",
    "\n",
    "        Q  = self.online_net(S).gather(1, A)\n",
    "        next_a = self.online_net(S2).argmax(dim=1, keepdim=True)\n",
    "        Q2 = self.target_net(S2).gather(1, next_a)\n",
    "        target = R + self.gamma * Q2 * (1 - D)\n",
    "\n",
    "        loss = nn.MSELoss()(Q, target.detach())\n",
    "        self.optimizer.zero_grad(); loss.backward(); self.optimizer.step()\n",
    "\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)\n",
    "\n",
    "    def step(self, s, a, r, s2, done=False):\n",
    "        self.store(s,a,r,s2,done)\n",
    "        self.update_network()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.online_net.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.online_net.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.target_net.load_state_dict(self.online_net.state_dict())\n",
    "        self.online_net.to(self.device); self.target_net.to(self.device)\n",
    "\n",
    "    def assert_on_device(self):\n",
    "        dev = self.device\n",
    "        ok = all(p.device == dev for p in self.online_net.parameters())\n",
    "        print(f\"[DDQN] Params on {dev}: {ok}\")\n",
    "        return ok\n",
    "\n",
    "# -------------------------\n",
    "# RL wrapper around GA\n",
    "# -------------------------\n",
    "def estimate_bandwidth_vector_RL(state, initial_bw, RL_agent, prev_ga_obj,\n",
    "                                 U_ref, X, C, m, con_idx, fac_idx, ord_idx,\n",
    "                                 max_iter_obj=1, verbose=False):\n",
    "    r_list = []\n",
    "\n",
    "    action_idx = RL_agent.choose_action(state)\n",
    "    p_m, p_c = index_to_action(action_idx)\n",
    "    if verbose:\n",
    "        print(f\"[DDQN] state={state} -> action p_m={p_m:.2f}, p_c={p_c:.2f}\")\n",
    "\n",
    "    best_bw, best_obj, final_pop = genetic_algorithm(\n",
    "        pop_size=20, num_generations=10, bandwidth_length=len(initial_bw),\n",
    "        p_m=p_m, p_c=p_c, tournament_size=3, sigma=5.0,\n",
    "        U_ref=U_ref, X=X, C=C, m=m, con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx,\n",
    "        verbose=False, initial_bw=initial_bw, max_iter_obj=max_iter_obj\n",
    "    )\n",
    "\n",
    "    diversity = compute_diversity(final_pop)\n",
    "    improvement = 0.0 if prev_ga_obj is None else (prev_ga_obj - best_obj)\n",
    "    next_state = (discretize_improvement(improvement), discretize_diversity(diversity))\n",
    "    reward = REWARD_TABLE[next_state]\n",
    "\n",
    "    RL_agent.step(state, action_idx, reward, next_state, done=False)\n",
    "    r_list.append(reward)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[DDQN] improvement={improvement:.6f} diversity={diversity:.4f} reward={reward}\")\n",
    "\n",
    "    return best_bw, best_obj, r_list, next_state\n",
    "\n",
    "def train_RL_agent(X, initial_bw, RL_agent, prev_ga_obj, U_ref, C, m,\n",
    "                   con_idx, fac_idx, ord_idx, B=200, max_iter_obj=1,\n",
    "                   ckpt_dir=\"./checkpoints\", verbose=False):\n",
    "    ensure_dir(ckpt_dir)\n",
    "    rewards_all = []\n",
    "    state = (STALLED, VLD)\n",
    "    bw_current = list(initial_bw)\n",
    "\n",
    "    for b in tqdm(range(B), desc=\"Training DDQN\"):\n",
    "        bw_current, ga_obj, r_list, state = estimate_bandwidth_vector_RL(\n",
    "            state=state, initial_bw=bw_current, RL_agent=RL_agent, prev_ga_obj=prev_ga_obj,\n",
    "            U_ref=U_ref, X=X, C=C, m=m, con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx,\n",
    "            max_iter_obj=max_iter_obj, verbose=verbose\n",
    "        )\n",
    "        rewards_all.extend(r_list)\n",
    "        prev_ga_obj = ga_obj\n",
    "\n",
    "        if (b+1) % 50 == 0:\n",
    "            RL_agent.save(os.path.join(ckpt_dir, \"ddqn_agent.pt\"))\n",
    "\n",
    "    RL_agent.save(os.path.join(ckpt_dir, \"ddqn_agent.pt\"))\n",
    "\n",
    "    # Plot cumulative reward\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(np.cumsum(rewards_all), marker='o')\n",
    "    plt.title(\"Cumulative Reward during DDQN Training\")\n",
    "    plt.xlabel(\"Episode\"); plt.ylabel(\"Cumulative Reward\"); plt.grid(True)\n",
    "    ensure_dir(\"./outputs\"); plt.savefig(\"./outputs/ddqn_training_rewards.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    return RL_agent, rewards_all\n",
    "\n",
    "# -------------------------\n",
    "# Full KDSS-FCM with RL-GA bandwidth search\n",
    "# -------------------------\n",
    "def centers_from_membership(X, U, m):\n",
    "    C = U.shape[1]; D = X.shape[1]\n",
    "    V = np.zeros((C, D))\n",
    "    for c in range(C):\n",
    "        w = (U[:, c] ** m).reshape(-1, 1)\n",
    "        num = np.sum(w * X, axis=0)\n",
    "        den = np.sum(U[:, c] ** m)\n",
    "        V[c] = num / max(den, 1e-12)\n",
    "    return V\n",
    "\n",
    "def kdml_fcm(X, C, m, epsilon, con_idx, fac_idx, ord_idx,\n",
    "             U_init, max_iter=100, verbose=False,\n",
    "             RL_agent=None, train_B=0, max_iter_obj=1, ckpt_dir=\"./checkpoints\"):\n",
    "    N, D = X.shape\n",
    "\n",
    "    # Ensure lists\n",
    "    con_idx = [] if con_idx is None else list(con_idx)\n",
    "    fac_idx = [] if fac_idx is None else list(fac_idx)\n",
    "    ord_idx = [] if ord_idx is None else list(ord_idx)\n",
    "\n",
    "    # Initialize fuzzy membership from provided init membership (normalize)\n",
    "    U = np.asarray(U_init, dtype=float)\n",
    "    U = U / np.clip(U.sum(axis=1, keepdims=True), 1e-12, None)\n",
    "\n",
    "    # Initialize centers from that membership\n",
    "    V = centers_from_membership(X, U, m)\n",
    "\n",
    "    # Random start bandwidths per feature type (fallback to one-per-col if mismatch)\n",
    "    bw_current = [np.random.uniform(0.1,100) for _ in con_idx] \\\n",
    "               + [np.random.uniform(0,1)   for _ in fac_idx] \\\n",
    "               + [np.random.uniform(0,1)   for _ in ord_idx]\n",
    "    if len(bw_current) != D:\n",
    "        bw_current = [np.random.uniform(0.1, 10.0) for _ in range(D)]\n",
    "        con_idx = list(range(D)); fac_idx=[]; ord_idx=[]\n",
    "\n",
    "    # Train / init agent\n",
    "    if RL_agent is None:\n",
    "        RL_agent = DDQNAgent(lr=1e-3, gamma=0.9, epsilon=1.0)\n",
    "\n",
    "    if train_B and train_B > 0:\n",
    "        RL_agent, _ = train_RL_agent(\n",
    "            X=X, initial_bw=bw_current, RL_agent=RL_agent, prev_ga_obj=None,\n",
    "            U_ref=U, C=C, m=m, con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx,\n",
    "            B=train_B, max_iter_obj=max_iter_obj, ckpt_dir=ckpt_dir, verbose=verbose\n",
    "        )\n",
    "\n",
    "    # RL loop over FCM iterations\n",
    "    J_prev = float('inf')\n",
    "    reward_history = []\n",
    "    state = RL_agent.get_state(0.0, 1.0)  # start state\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        if verbose:\n",
    "            print(f\"\\n[FCM] iter={it}\")\n",
    "\n",
    "        # One RL-GA step to update bandwidth\n",
    "        bw_current, ga_obj, r_list, state = estimate_bandwidth_vector_RL(\n",
    "            state=state, initial_bw=bw_current, RL_agent=RL_agent, prev_ga_obj=None,\n",
    "            U_ref=U, X=X, C=C, m=m, con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx,\n",
    "            max_iter_obj=max_iter_obj, verbose=verbose\n",
    "        )\n",
    "        reward_history.extend(r_list)\n",
    "\n",
    "        # Update centers from current U\n",
    "        V = centers_from_membership(X, U, m)\n",
    "\n",
    "        # Distances with new bw\n",
    "        Dmat = np.zeros((N, C))\n",
    "        for i in range(N):\n",
    "            for c in range(C):\n",
    "                Dmat[i, c] = dkss_distance(\n",
    "                    X[i], V[c], np.asarray(bw_current),\n",
    "                    cFUN=\"c_gaussian\", uFUN=\"u_aitken\", oFUN=\"o_wangvanryzin\",\n",
    "                    con_idx=con_idx, fac_idx=fac_idx, ord_idx=ord_idx\n",
    "                )\n",
    "\n",
    "        # Update U\n",
    "        for i in range(N):\n",
    "            for c in range(C):\n",
    "                if Dmat[i, c] == 0:\n",
    "                    U[i, :] = 0; U[i, c] = 1\n",
    "                else:\n",
    "                    denom = 0.0\n",
    "                    for j in range(C):\n",
    "                        denom += (Dmat[i, c] / (Dmat[i, j] if Dmat[i, j] > 0 else 1e-12)) ** (2.0 / (m - 1))\n",
    "                    U[i, c] = 1.0 / max(denom, 1e-12)\n",
    "\n",
    "        J = np.sum((U ** m) * (Dmat ** 2))\n",
    "        if verbose:\n",
    "            print(f\"[FCM] J={J:.6f}  Δ={abs(J-J_prev):.3e}  bw[0..3]={np.asarray(bw_current)[:3]}\")\n",
    "        if abs(J - J_prev) < epsilon:\n",
    "            if verbose: print(f\"[FCM] Converged at iter {it}\")\n",
    "            break\n",
    "        J_prev = J\n",
    "\n",
    "    # Plot cumulative reward\n",
    "    ensure_dir(\"./outputs\")\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(np.cumsum(reward_history))\n",
    "    plt.title(\"Cumulative Reward during KDSS-FCM (DDQN-controlled)\")\n",
    "    plt.xlabel(\"Iteration\"); plt.ylabel(\"Cumulative Reward\"); plt.grid(True)\n",
    "    plt.savefig(\"./outputs/fcm_cumulative_reward.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    return V, U, reward_history, np.asarray(bw_current)\n",
    "\n",
    "# =========================================================================================\n",
    "# === Train DDQN-GA KDSS-FCM using your CSVs (first 3 cols are data: 2 continuous + 1 ordinal)\n",
    "# =========================================================================================\n",
    "\n",
    "# 1) File paths you provided\n",
    "DATA_CSV       = \"./data/3D 2 Cont 1 Ord C=2/CUSTOM_G_mean_-3_0_3_sd_1_1_1_Ga_a_2_4_6_b_1.5_1.2_0.9_Pois_2_6_10.csv\"\n",
    "INIT_MEMB_CSV  = \"./data/3D 2 Cont 1 Ord C=2/CUSTOM_G_mean_-3_0_3_sd_1_1_1_Ga_a_2_4_6_b_1.5_1.2_0.9_Pois_2_6_10_membership.csv\"\n",
    "TRUE_POST_CSV  = \"./data/3D 2 Cont 1 Ord C=2/CUSTOM_G_mean_-3_0_3_sd_1_1_1_Ga_a_2_4_6_b_1.5_1.2_0.9_Pois_2_6_10_true_posterior.csv\"\n",
    "\n",
    "# 2) Mixed-type indices (0-based): first two continuous, last is ordinal\n",
    "con_idx = [0, 1]\n",
    "fac_idx = None\n",
    "ord_idx = [2]\n",
    "\n",
    "# 3) Hyperparameters\n",
    "m              = 1.2\n",
    "epsilon        = 1e-3\n",
    "max_iters      = 100          # outer FCM iterations\n",
    "max_iter_obj   = 1            # inner FCM lookahead for GA objective\n",
    "train_B        = 150          # DDQN pre-training episodes (GA-only); set 0 to skip\n",
    "verbose        = True\n",
    "ckpt_dir       = \"./checkpoints\"\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# 4) Load CSVs\n",
    "df_full  = pd.read_csv(DATA_CSV)\n",
    "df       = df_full.iloc[:, :3].copy()           # only the first 3 columns are data\n",
    "U_init   = pd.read_csv(INIT_MEMB_CSV).to_numpy(dtype=float)\n",
    "U_true   = pd.read_csv(TRUE_POST_CSV).to_numpy(dtype=float)\n",
    "\n",
    "print(f\"Data shape (raw/used): {df_full.shape} / {df.shape}\")\n",
    "print(f\"Init membership shape: {U_init.shape}\")\n",
    "print(f\"True posterior shape : {U_true.shape}\")\n",
    "\n",
    "# 5) Encode mixed dataframe -> numeric matrix for kernels\n",
    "X = encode_mixed_df(df, fac_idx=fac_idx, ord_idx=ord_idx).to_numpy(dtype=float)\n",
    "\n",
    "# 6) Shape checks / cluster count\n",
    "n = X.shape[0]\n",
    "if U_init.shape[0] != n or U_true.shape[0] != n:\n",
    "    raise ValueError(f\"Row mismatch: data n={n}, init_U n={U_init.shape[0]}, true_post n={U_true.shape[0]}\")\n",
    "C = U_init.shape[1]  # infer number of clusters\n",
    "print(f\"Inferred clusters C = {C}\")\n",
    "\n",
    "# 7) Create / load DDQN agent with device\n",
    "device = pick_device()\n",
    "print(f\"[System] Selected device: {device}\")\n",
    "agent = DDQNAgent(lr=1e-3, gamma=0.9, epsilon=1.0, device=device)\n",
    "agent.assert_on_device()\n",
    "\n",
    "agent_path = os.path.join(ckpt_dir, \"ddqn_agent.pt\")\n",
    "agent.load(agent_path)\n",
    "# if os.path.exists(agent_path):\n",
    "#     try:\n",
    "#         agent.load(agent_path)\n",
    "#         print(f\"[Agent] Loaded from {agent_path}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"[Agent] Could not load checkpoint ({e}); starting fresh.\")\n",
    "\n",
    "# Optional quick GPU/MPS runtime check\n",
    "s_test = torch.from_numpy(agent._encode(agent.get_state(0.0, 1.0))).unsqueeze(0).to(agent.device)\n",
    "with torch.no_grad():\n",
    "    q_test = agent.online_net(s_test)\n",
    "print(\"[Check] Q tensor device:\", q_test.device)\n",
    "\n",
    "# 8) Run KDSS-FCM with DDQN-controlled GA\n",
    "V_final, U_final, reward_hist, bw_final = kdml_fcm(\n",
    "    X=X,\n",
    "    C=C,\n",
    "    m=m,\n",
    "    epsilon=epsilon,\n",
    "    con_idx=con_idx,\n",
    "    fac_idx=fac_idx,\n",
    "    ord_idx=ord_idx,\n",
    "    U_init=U_init,\n",
    "    max_iter=max_iters,\n",
    "    verbose=verbose,\n",
    "    RL_agent=agent,\n",
    "    train_B=train_B,          # set to 0 to skip pre-training\n",
    "    max_iter_obj=max_iter_obj,\n",
    "    ckpt_dir=ckpt_dir\n",
    ")\n",
    "\n",
    "# Save agent checkpoint\n",
    "agent.save(agent_path)\n",
    "print(f\"[Agent] Saved to {agent_path}\")\n",
    "\n",
    "# 9) Evaluate and persist outputs\n",
    "fari_vs_truth = fari(U_final, U_true)\n",
    "fari_vs_init  = fari(U_final, U_init)\n",
    "print(\"\\n=== RESULTS (Your CSVs) ===\")\n",
    "print(f\"FARI(U_final, true_post) = {fari_vs_truth:.6f}\")\n",
    "print(f\"FARI(U_final, init_U)    = {fari_vs_init:.6f}\")\n",
    "print(f\"Final bandwidth (all): {np.asarray(bw_final)}\")\n",
    "\n",
    "np.savetxt(\"./outputs/U_final.csv\", U_final, delimiter=\",\")\n",
    "np.savetxt(\"./outputs/V_final.csv\", V_final, delimiter=\",\")\n",
    "np.savetxt(\"./outputs/bw_final.csv\", np.asarray(bw_final), delimiter=\",\")\n",
    "print(\"Saved: ./outputs/U_final.csv, ./outputs/V_final.csv, ./outputs/bw_final.csv\")\n",
    "\n",
    "# 10) Reward plot\n",
    "cum = np.cumsum(reward_hist)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(cum, marker='o')\n",
    "plt.title(\"Cumulative Reward during KDSS-FCM (DDQN-controlled)\")\n",
    "plt.xlabel(\"FCM iteration\")\n",
    "plt.ylabel(\"Cumulative reward\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81c9845-1441-4d68-811d-ecb27e45feea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Membership matrix:\n",
      "   cluster_1  cluster_2  cluster_3\n",
      "0          1          0          0\n",
      "1          0          1          0\n",
      "2          1          0          0\n",
      "3          0          1          0\n",
      "4          0          0          1 \n",
      "\n",
      "True posterior memberships:\n",
      "      cluster_1  cluster_2     cluster_3\n",
      "0  9.940605e-01   0.005939  2.326022e-09\n",
      "1  9.999510e-01   0.000049  1.245670e-13\n",
      "2  2.080393e-04   0.993395  6.396589e-03\n",
      "3  9.994352e-01   0.000565  1.127854e-11\n",
      "4  7.736371e-12   0.009033  9.909666e-01 \n",
      "\n",
      "Data:\n",
      "   X_gauss_1  X_gamma_1  Y_pois_1  TrueCluster  PredCluster\n",
      "0  -1.665087   1.087544         1            1            1\n",
      "1  -3.869272   3.207869         1            1            2\n",
      "2   0.055487   9.590700         6            2            1\n",
      "3  -2.950933   1.359228         2            1            2\n",
      "4   2.421644   7.116113        11            3            3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "membership_path = \"data/3D 2 Cont 1 Ord C=2/CUSTOM_G_mean_-3_0_3_sd_1_1_1_Ga_a_2_4_6_b_1.5_1.2_0.9_Pois_2_6_10_membership.csv\"\n",
    "true_post_path = \"data/3D 2 Cont 1 Ord C=2/CUSTOM_G_mean_-3_0_3_sd_1_1_1_Ga_a_2_4_6_b_1.5_1.2_0.9_Pois_2_6_10_true_posterior.csv\"\n",
    "data_path = \"data/3D 2 Cont 1 Ord C=2/CUSTOM_G_mean_-3_0_3_sd_1_1_1_Ga_a_2_4_6_b_1.5_1.2_0.9_Pois_2_6_10.csv\"\n",
    "\n",
    "# Load datasets\n",
    "membership_df = pd.read_csv(membership_path)\n",
    "true_post_df = pd.read_csv(true_post_path)\n",
    "data_df = pd.read_csv(data_path)\n",
    "\n",
    "# Print heads\n",
    "print(\"Membership matrix:\")\n",
    "print(membership_df.head(), \"\\n\")\n",
    "\n",
    "print(\"True posterior memberships:\")\n",
    "print(true_post_df.head(), \"\\n\")\n",
    "\n",
    "print(\"Data:\")\n",
    "print(data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63330b24-5762-4c25-a773-b11d89d5d48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
